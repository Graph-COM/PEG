{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d23d6e47",
   "metadata": {},
   "source": [
    "# Link prediction example for PEG (cora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1dce0a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70a94471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from peg_conv import PEGConv\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72cdfd8",
   "metadata": {},
   "source": [
    "# Download dataset and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7688549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = f'cuda:{1}' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf1b676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "dataset = 'Cora'\n",
    "path = osp.join('.', 'data', dataset)\n",
    "dataset = Planetoid(path, dataset, transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "print(dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2da1343c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch1/wang5386/py2021/lib/python3.7/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], val_pos_edge_index=[2, 263], test_pos_edge_index=[2, 527], train_pos_edge_index=[2, 8976], train_neg_adj_mask=[2708, 2708], val_neg_edge_index=[2, 263], test_neg_edge_index=[2, 527])\n"
     ]
    }
   ],
   "source": [
    "data.train_mask = data.val_mask = data.test_mask = data.y = None\n",
    "data = train_test_split_edges(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b72f2c",
   "metadata": {},
   "source": [
    "# Preprocessing: calculate positional encoding (Deepwalk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8130a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build train matrix for PE preparation\n",
    "import copy\n",
    "train_graph = copy.deepcopy(dataset[0])\n",
    "train_graph.edge_index = data.train_pos_edge_index\n",
    "G = to_networkx(train_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1e45605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import math\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54e92f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_num(num, workers):\n",
    "    if num % workers == 0:\n",
    "        return [num//workers]*workers\n",
    "    else:\n",
    "        return [num//workers]*workers + [num % workers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c76682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified from https://github.com/shenweichen/GraphEmbedding\n",
    "class RandomWalker:\n",
    "    def __init__(self, G, p=1, q=1, use_rejection_sampling=0):\n",
    "        \"\"\"\n",
    "        :param G:\n",
    "        :param p: Return parameter,controls the likelihood of immediately revisiting a node in the walk.\n",
    "        :param q: In-out parameter,allows the search to differentiate between “inward” and “outward” nodes\n",
    "        :param use_rejection_sampling: Whether to use the rejection sampling strategy in node2vec.\n",
    "        \"\"\"\n",
    "        self.G = G\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.use_rejection_sampling = use_rejection_sampling\n",
    "    \n",
    "    def deepwalk_walk(self, walk_length, start_node):\n",
    "\n",
    "        walk = [start_node]\n",
    "\n",
    "        while len(walk) < walk_length:\n",
    "            cur = walk[-1]\n",
    "            cur_nbrs = list(self.G.neighbors(cur))\n",
    "            if len(cur_nbrs) > 0:\n",
    "                walk.append(random.choice(cur_nbrs))\n",
    "            else:\n",
    "                break\n",
    "        return walk\n",
    "\n",
    "    def simulate_walks(self, num_walks, walk_length, workers=1, verbose=0):\n",
    "\n",
    "        G = self.G\n",
    "\n",
    "        nodes = list(G.nodes())\n",
    "\n",
    "        results = Parallel(n_jobs=workers, verbose=verbose, )(\n",
    "            delayed(self._simulate_walks)(nodes, num, walk_length) for num in\n",
    "            partition_num(num_walks, workers))\n",
    "\n",
    "        walks = list(itertools.chain(*results))\n",
    "\n",
    "        return walks\n",
    "    \n",
    "    def _simulate_walks(self, nodes, num_walks, walk_length,):\n",
    "        walks = []\n",
    "        for _ in range(num_walks):\n",
    "            random.shuffle(nodes)\n",
    "            for v in nodes:\n",
    "                if self.p == 1 and self.q == 1:\n",
    "                    walks.append(self.deepwalk_walk(\n",
    "                        walk_length=walk_length, start_node=v))\n",
    "                else:\n",
    "                    return (\"only work for DeepWalk\")\n",
    "        return walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21177eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "\n",
    "class DeepWalk:\n",
    "    def __init__(self, graph, walk_length = 80, num_walks = 10, workers=1):\n",
    "\n",
    "        self.graph = graph\n",
    "        self.w2v_model = None\n",
    "        self._embeddings = {}\n",
    "\n",
    "        self.walker = RandomWalker(\n",
    "            graph, p=1, q=1, )\n",
    "        self.sentences = self.walker.simulate_walks(\n",
    "            num_walks=num_walks, walk_length=walk_length, workers=workers, verbose=1)\n",
    "\n",
    "    def train(self, embed_size=128, window_size=5, workers=3, iter=3, **kwargs):\n",
    "\n",
    "        kwargs[\"sentences\"] = self.sentences\n",
    "        kwargs[\"min_count\"] = kwargs.get(\"min_count\", 0)\n",
    "        kwargs[\"vector_size\"] = embed_size\n",
    "        kwargs[\"sg\"] = 1  # skip gram\n",
    "        kwargs[\"hs\"] = 1  # deepwalk use Hierarchical Softmax\n",
    "        kwargs[\"workers\"] = workers\n",
    "        kwargs[\"window\"] = window_size\n",
    "        kwargs[\"epochs\"] = iter\n",
    "\n",
    "        print(\"Learning embedding vectors...\")\n",
    "        model = Word2Vec(**kwargs)\n",
    "        print(\"Learning embedding vectors done!\")\n",
    "\n",
    "        self.w2v_model = model\n",
    "        return model\n",
    "\n",
    "    def get_embeddings(self,):\n",
    "        if self.w2v_model is None:\n",
    "            print(\"model not train\")\n",
    "            return {}\n",
    "\n",
    "        self._embeddings = {}\n",
    "        for word in self.graph.nodes():\n",
    "            self._embeddings[word] = self.w2v_model.wv[word]\n",
    "\n",
    "        return self._embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ed91ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning embedding vectors...\n",
      "Learning embedding vectors done!\n"
     ]
    }
   ],
   "source": [
    "model_emb = DeepWalk(G,walk_length=80, num_walks=10,workers=1)#init model\n",
    "model_emb.train(embed_size = 128)# train model\n",
    "emb = model_emb.get_embeddings()# get embedding vectors\n",
    "embeddings = []\n",
    "for i in range(len(emb)):\n",
    "    embeddings.append(emb[i])\n",
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bd9cb9",
   "metadata": {},
   "source": [
    "# Training: Set up model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6efbbd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_feats_dim, hidden_dim):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.in_feats_dim = in_feats_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.conv1 = PEGConv(in_channels = in_feats_dim, out_channels = hidden_dim)\n",
    "        self.conv2 = PEGConv(in_channels = hidden_dim, out_channels = hidden_dim)\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        self.fc = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x, pos, pos_edge_index, neg_edge_index):\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1)\n",
    "        x = self.conv1(x, pos, pos_edge_index)\n",
    "        x = self.conv2(x, pos, pos_edge_index)\n",
    "        \n",
    "        nodes_first = x[edge_index[0]]\n",
    "        nodes_second = x[edge_index[1]]\n",
    "        pos_first = pos[edge_index[0]]\n",
    "        pos_second = pos[edge_index[1]]\n",
    "        \n",
    "        positional_encoding = ((pos_first - pos_second)**2).sum(dim=-1, keepdim=True)\n",
    "\n",
    "        pred = (nodes_first * nodes_second).sum(dim=-1)  # dot product \n",
    "        out = self.fc(torch.cat([pred.reshape(len(pred), 1),positional_encoding.reshape(len(positional_encoding), 1)], 1))\n",
    "\n",
    "        return out\n",
    "\n",
    "    def loss(self, pred, link_label):\n",
    "        return self.loss_fn(pred, link_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857bbae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a153e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.x\n",
    "pos = torch.tensor(embeddings).to(device)\n",
    "x = x.cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e9b0010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_labels(pos_edge_index, neg_edge_index):\n",
    "    # returns a tensor:\n",
    "    # [1,1,1,1,...,0,0,0,0,0,..] with the number of ones is equel to the lenght of pos_edge_index\n",
    "    # and the number of zeros is equal to the length of neg_edge_index\n",
    "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
    "    link_labels = torch.zeros(E, dtype=torch.float, device=device)\n",
    "    link_labels[:pos_edge_index.size(1)] = 1.\n",
    "    return link_labels\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.train_pos_edge_index, #positive edges\n",
    "        num_nodes=data.num_nodes, # number of nodes\n",
    "        num_neg_samples=data.train_pos_edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    link_logits = model(x, pos, data.train_pos_edge_index, neg_edge_index) # decode\n",
    "    link_logits = link_logits.reshape(len(link_logits),)\n",
    "    link_labels = get_link_labels(data.train_pos_edge_index, neg_edge_index)\n",
    "    loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        model.fc.weight[0][0].clamp_(1e-5,100)\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    perfs = []\n",
    "    for prefix in [\"val\", \"test\"]:\n",
    "        pos_edge_index = data[f'{prefix}_pos_edge_index']\n",
    "        neg_edge_index = data[f'{prefix}_neg_edge_index']\n",
    "\n",
    "        link_logits = model(x, pos, pos_edge_index, neg_edge_index) # decode test or val\n",
    "        \n",
    "        link_probs = link_logits.sigmoid() # apply sigmoid\n",
    "        \n",
    "        link_labels = get_link_labels(pos_edge_index, neg_edge_index) # get link\n",
    "        \n",
    "        perfs.append(roc_auc_score(link_labels.cpu(), link_probs.cpu())) #compute roc_auc score\n",
    "    return perfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9112a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(in_feats_dim = dataset.num_features, hidden_dim = 128)\n",
    "data = data.to(device) \n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 0.01, weight_decay= 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8882dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 0.2796, Val: 0.9022, Test: 0.9170\n",
      "Epoch: 020, Loss: 0.0455, Val: 0.9022, Test: 0.9170\n",
      "Epoch: 030, Loss: 0.0514, Val: 0.9022, Test: 0.9170\n",
      "Epoch: 040, Loss: 0.0527, Val: 0.9022, Test: 0.9170\n",
      "Epoch: 050, Loss: 0.0427, Val: 0.9022, Test: 0.9170\n",
      "Epoch: 060, Loss: 0.0392, Val: 0.9022, Test: 0.9170\n",
      "Epoch: 070, Loss: 0.0381, Val: 0.9022, Test: 0.9170\n",
      "Epoch: 080, Loss: 0.0330, Val: 0.9022, Test: 0.9170\n",
      "Epoch: 090, Loss: 0.0333, Val: 0.9022, Test: 0.9170\n",
      "Epoch: 100, Loss: 0.0333, Val: 0.9022, Test: 0.9170\n"
     ]
    }
   ],
   "source": [
    "best_val_perf = test_perf = 0\n",
    "for epoch in range(1, 101):\n",
    "    train_loss = train()\n",
    "    val_perf, tmp_test_perf = test()\n",
    "    if val_perf > best_val_perf:\n",
    "        best_val_perf = val_perf\n",
    "        test_perf = tmp_test_perf\n",
    "    log = 'Epoch: {:03d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    if epoch % 10 == 0:\n",
    "        print(log.format(epoch, train_loss, best_val_perf, test_perf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb0b526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3038114f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
